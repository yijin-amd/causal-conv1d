# SiLU Activation æ”¯æŒ

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜äº†åœ¨ Causal Conv1D å®ç°ä¸­æ·»åŠ  SiLU (Sigmoid Linear Unit) activation å‡½æ•°çš„å®ç°æ–¹æ¡ˆã€‚

---

## ä»€ä¹ˆæ˜¯ SiLUï¼Ÿ

### æ•°å­¦å®šä¹‰

**SiLU** (Sigmoid Linear Unit)ï¼Œä¹Ÿè¢«ç§°ä¸º **Swish**ï¼Œæ˜¯ä¸€ç§å¹³æ»‘çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼š

```
SiLU(x) = x Â· sigmoid(x) = x Â· (1 / (1 + e^(-x)))
```

### å‡½æ•°ç‰¹ç‚¹

1. **å¹³æ»‘å¯å¾®**ï¼šåœ¨æ•´ä¸ªå®šä¹‰åŸŸä¸Šè¿ç»­ä¸”å¯å¾®
2. **éå•è°ƒ**ï¼šä¸ ReLU ä¸åŒï¼ŒSiLU åœ¨è´Ÿå€¼åŒºåŸŸä¸æ˜¯æ’ä¸º 0
3. **è‡ªé—¨æ§**ï¼šé€šè¿‡ sigmoid å‡½æ•°å®ç°è‡ªé€‚åº”çš„é—¨æ§æœºåˆ¶
4. **æ€§èƒ½ä¼˜å¼‚**ï¼šåœ¨è®¸å¤šæ·±åº¦å­¦ä¹ ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äº ReLU

### å‡½æ•°å›¾åƒç‰¹æ€§

- å½“ `x â†’ +âˆ` æ—¶ï¼Œ`SiLU(x) â†’ x` (çº¿æ€§å¢é•¿)
- å½“ `x â†’ -âˆ` æ—¶ï¼Œ`SiLU(x) â†’ 0` (è¶‹è¿‘äº 0)
- åœ¨ `x = 0` é™„è¿‘å¹³æ»‘è¿‡æ¸¡

---

## å®ç°æ–¹æ¡ˆ

### æ¶æ„é€‰æ‹©ï¼šèåˆ Kernel

ä¸ºäº†æœ€å¤§åŒ–æ€§èƒ½ï¼Œæˆ‘ä»¬å°† **bias åŠ æ³•** å’Œ **SiLU activation** èåˆåˆ°å•ä¸ª GPU kernel ä¸­ï¼š

```
ä¼ ç»Ÿæ–¹æ¡ˆï¼ˆä¸¤æ¬¡å†…å­˜è®¿é—®ï¼‰:
  1. add_bias_kernel: C[h,c] += bias[c]
  2. silu_kernel: C[h,c] = SiLU(C[h,c])

èåˆæ–¹æ¡ˆï¼ˆä¸€æ¬¡å†…å­˜è®¿é—®ï¼‰:
  add_bias_silu_fused_kernel: 
    x = C[h,c] + bias[c]
    C[h,c] = x / (1 + exp(-x))
```

**æ€§èƒ½ä¼˜åŠ¿ï¼š**
- âœ… å‡å°‘å†…å­˜è¯»å†™æ¬¡æ•°ï¼šä» 4 æ¬¡ï¼ˆè¯»-å†™-è¯»-å†™ï¼‰é™ä½åˆ° 2 æ¬¡ï¼ˆè¯»-å†™ï¼‰
- âœ… æé«˜ç¼“å­˜å‘½ä¸­ç‡ï¼šä¸­é—´ç»“æœæ— éœ€å†™å›æ˜¾å­˜
- âœ… å‡å°‘ kernel å¯åŠ¨å¼€é”€ï¼šä» 2 æ¬¡ kernel è°ƒç”¨é™ä½åˆ° 1 æ¬¡

---

## GPU Kernel å®ç°

### 1. èåˆ Kernelï¼ˆæ¨èï¼‰

```cpp
__global__ void add_bias_silu_fused_kernel(
    fp16_t* __restrict__ output,       // è¾“å‡º [ho, ci] (in-place)
    const fp16_t* __restrict__ bias,   // bias [ci]
    int ho, int ci)
{
    int h = blockIdx.x * blockDim.x + threadIdx.x;  // è¾“å‡ºä½ç½® [0, ho)
    int c = blockIdx.y * blockDim.y + threadIdx.y;  // channel [0, ci)
    
    if (h >= ho || c >= ci) return;
    
    int idx = h * ci + c;
    
    // æ­¥éª¤1: æ·»åŠ  bias
    float x = (float)output[idx] + (float)bias[c];
    
    // æ­¥éª¤2: åº”ç”¨ SiLU activation
    // SiLU(x) = x * sigmoid(x) = x / (1 + exp(-x))
    float sigmoid_x = 1.0f / (1.0f + expf(-x));
    float silu_x = x * sigmoid_x;
    
    output[idx] = (fp16_t)silu_x;
}
```

**å®ç°ç»†èŠ‚ï¼š**

1. **FP16 â†’ FP32 è½¬æ¢**ï¼š
   - è¾“å…¥å’Œ bias ä» FP16 è½¬æ¢ä¸º FP32 è¿›è¡Œè®¡ç®—
   - é¿å… FP16 ç²¾åº¦æŸå¤±ï¼ˆç‰¹åˆ«æ˜¯åœ¨ `exp` è®¡ç®—ä¸­ï¼‰

2. **æ•°å­¦ç¨³å®šæ€§**ï¼š
   - ä½¿ç”¨ `expf(-x)` è€Œé `exp(-x)`ï¼ˆFP32 æŒ‡æ•°å‡½æ•°ï¼‰
   - å¯¹äºå¤§çš„æ­£å€¼ `x`ï¼Œ`exp(-x)` æ¥è¿‘ 0ï¼Œ`sigmoid(x)` æ¥è¿‘ 1
   - å¯¹äºå¤§çš„è´Ÿå€¼ `x`ï¼Œ`exp(-x)` å¾ˆå¤§ï¼Œä½†ä¸ä¼šæº¢å‡ºï¼ˆåˆ†æ¯èµ·ä¿æŠ¤ä½œç”¨ï¼‰

3. **å†…å­˜è®¿é—®æ¨¡å¼**ï¼š
   - çº¿ç¨‹ `(h, c)` è®¿é—® `output[h * ci + c]`ï¼šè¿ç»­è®¿é—®åŒä¸€è¡Œçš„å…ƒç´ 
   - çº¿ç¨‹å—å†…åˆå¹¶å†…å­˜è®¿é—®ï¼ˆcoalesced accessï¼‰
   - Bias å¹¿æ’­ï¼šæ‰€æœ‰å¤„ç†ç›¸åŒ channel çš„çº¿ç¨‹å…±äº« `bias[c]`

### 2. ç‹¬ç«‹ SiLU Kernelï¼ˆå¤‡é€‰ï¼‰

å¦‚æœéœ€è¦å•ç‹¬çš„ SiLU kernelï¼ˆä¾‹å¦‚ï¼Œåœ¨æ²¡æœ‰ bias çš„æƒ…å†µä¸‹ï¼‰ï¼š

```cpp
__global__ void silu_activation_kernel(
    fp16_t* __restrict__ output,       // è¾“å‡º [ho, ci] (in-place)
    int ho, int ci)
{
    int h = blockIdx.x * blockDim.x + threadIdx.x;
    int c = blockIdx.y * blockDim.y + threadIdx.y;
    
    if (h >= ho || c >= ci) return;
    
    int idx = h * ci + c;
    float x = (float)output[idx];
    
    // SiLU(x) = x * sigmoid(x) = x / (1 + exp(-x))
    float sigmoid_x = 1.0f / (1.0f + expf(-x));
    float silu_x = x * sigmoid_x;
    
    output[idx] = (fp16_t)silu_x;
}
```

---

## Host ç«¯å‚è€ƒå®ç°

### Depthwise Conv1D + SiLU

åœ¨ `causal_conv1d_depthwise` å‡½æ•°ä¸­æ·»åŠ  SiLUï¼š

```cpp
void causal_conv1d_depthwise(
    const float* input, const float* weight, const float* bias, float* output,
    int N, int C, int H, int kernel_size)
{
    int pad = kernel_size - 1;
    int H_pad = H + pad;
    
    // 1. Padding
    std::vector<float> padded(N * C * H_pad, 0.0f);
    for (int n = 0; n < N; ++n) {
        for (int c = 0; c < C; ++c) {
            for (int h = 0; h < H; ++h) {
                padded[n * C * H_pad + c * H_pad + pad + h] =
                    input[n * C * H + c * H + h];
            }
        }
    }
    
    // 2. Convolution + Bias + SiLU
    for (int n = 0; n < N; ++n) {
        for (int c = 0; c < C; ++c) {
            for (int t = 0; t < H; ++t) {
                // å·ç§¯
                float sum = bias ? bias[c] : 0.0f;
                for (int k = 0; k < kernel_size; ++k) {
                    float val = padded[n * C * H_pad + c * H_pad + t + k];
                    float w = weight[c * kernel_size + k];
                    sum += val * w;
                }
                
                // SiLU activation
                float sigmoid_x = 1.0f / (1.0f + expf(-sum));
                float silu_x = sum * sigmoid_x;
                
                output[n * C * H + c * H + t] = silu_x;
            }
        }
    }
}
```

---

## Kernel è°ƒç”¨

### GPU æ‰§è¡Œæµç¨‹

```cpp
// åœ¨ casual_conv1d_block_run å‡½æ•°ä¸­

// ========== æ‰§è¡Œ GEMM ==========
for (int b = 0; b < batch; b++) {
    matrix_core_kernel_block_v2<<<gdim, 256>>>(
        dev_a + b*lda*m,    // A [ho, hk*ci]
        dev_b,              // B [hk*ci, ci]
        dev_c + b*ldc*m,    // C [ho, ci]
        k, lda, ldb, ldc);
}

// ========== æ·»åŠ  Bias + SiLU (èåˆ) ==========
dim3 bias_block_dim(16, 16);
dim3 bias_grid_dim((ho + 15) / 16, (ci + 15) / 16);

for (int b = 0; b < batch; b++) {
    add_bias_silu_fused_kernel<<<bias_grid_dim, bias_block_dim>>>(
        reinterpret_cast<fp16_t*>(dev_c + b*ldc*m),  // [ho, ci]
        reinterpret_cast<fp16_t*>(dev_bias),          // [ci]
        ho, ci);
}
```

### çº¿ç¨‹é…ç½®

| å‚æ•° | å€¼ | è¯´æ˜ |
|------|-----|------|
| `block_dim` | `(16, 16)` | æ¯ä¸ªçº¿ç¨‹å— 256 ä¸ªçº¿ç¨‹ |
| `grid_dim` | `((ho+15)/16, (ci+15)/16)` | è¦†ç›–æ•´ä¸ªè¾“å‡ºçŸ©é˜µ |
| æ¯ä¸ªçº¿ç¨‹ | 1 ä¸ªè¾“å‡ºå…ƒç´  | `(h, c)` å¤„ç† `output[h, c]` |

---

## æµ‹è¯•éªŒè¯

### éªŒè¯æµç¨‹

1. **Host ç«¯å‚è€ƒè®¡ç®—**ï¼š
   - æ‰§è¡Œå®Œæ•´çš„ Conv1D + Bias + SiLUï¼ˆCPUï¼‰

2. **GPU è®¡ç®—**ï¼š
   - GEMM â†’ Add Bias + SiLU (fused kernel)

3. **ç»“æœå¯¹æ¯”**ï¼š
   - ä½¿ç”¨ `valid_vector` å‡½æ•°æ¯”è¾ƒ Host å’Œ GPU ç»“æœ
   - å®¹å·®ï¼š`nrms < 1e-3`

### æµ‹è¯•ç»“æœ

#### Batch = 1
```
åœ¨ GPU ä¸Šæ·»åŠ  bias å¹¶åº”ç”¨ SiLU activation (batch=1)...
âœ“ bias + SiLU activation å®Œæˆ
[batch=1, 2048x64x256, block_gemm_32x32x16_2x2x1_16x16x16], valid
âœ“ GPU GEMM è¾“å‡ºéªŒè¯é€šè¿‡! (batch 0)
âœ“ GPU è¾“å‡ºéªŒè¯é€šè¿‡! (batch 0)
```

#### Batch = 2
```
åœ¨ GPU ä¸Šæ·»åŠ  bias å¹¶åº”ç”¨ SiLU activation (batch=2)...
âœ“ bias + SiLU activation å®Œæˆ
[batch=2, 2048x64x256, block_gemm_32x32x16_2x2x1_16x16x16], valid
âœ“ GPU GEMM è¾“å‡ºéªŒè¯é€šè¿‡! (batch 0)
âœ“ GPU è¾“å‡ºéªŒè¯é€šè¿‡! (batch 0)
âœ“ GPU GEMM è¾“å‡ºéªŒè¯é€šè¿‡! (batch 1)
âœ“ GPU è¾“å‡ºéªŒè¯é€šè¿‡! (batch 1)
```

**ç»“è®ºï¼š** âœ… SiLU activation å®ç°æ­£ç¡®ï¼Œæ•°å€¼éªŒè¯é€šè¿‡ã€‚

---

## æ€§èƒ½è€ƒé‡

### è®¡ç®—å¤æ‚åº¦

å¯¹äºè¾“å‡º `[batch, ho, ci]`ï¼š

| æ“ä½œ | è®¡ç®—é‡ | å†…å­˜è®¿é—® |
|------|--------|---------|
| GEMM | `O(batch Â· ho Â· ci Â· k)` | è¯» A, Bï¼Œå†™ C |
| Bias + SiLU | `O(batch Â· ho Â· ci)` | è¯» C, biasï¼Œå†™ C |

**SiLU é¢å¤–å¼€é”€ï¼š**
- 1 æ¬¡ `expf` è°ƒç”¨ï¼ˆçº¦ 10-20 FLOPsï¼‰
- 1 æ¬¡é™¤æ³•
- 1 æ¬¡ä¹˜æ³•
- æ€»è®¡ï¼šæ¯å…ƒç´ çº¦ 25 FLOPs

### æ€§èƒ½å½±å“

å‡è®¾ `ho=2048, ci=64, k=256, batch=1`ï¼š

- **GEMM è®¡ç®—é‡**ï¼š`2048 Ã— 64 Ã— 256 Ã— 2 â‰ˆ 67M FLOPs`
- **SiLU è®¡ç®—é‡**ï¼š`2048 Ã— 64 Ã— 25 â‰ˆ 3.3M FLOPs`
- **SiLU å æ¯”**ï¼š`3.3M / 67M â‰ˆ 5%`

**ç»“è®ºï¼š** SiLU çš„è®¡ç®—å¼€é”€ç›¸å¯¹äº GEMM å¾ˆå°ï¼ˆ< 5%ï¼‰ï¼Œä¸”èåˆ kernel è¿›ä¸€æ­¥å‡å°‘äº†å†…å­˜è®¿é—®å¼€é”€ã€‚

### ä¼˜åŒ–å»ºè®®

1. **èåˆåˆ° GEMM**ï¼š
   - å°† SiLU ç›´æ¥èåˆåˆ° GEMM kernel çš„å†™å›é˜¶æ®µ
   - è¿›ä¸€æ­¥å‡å°‘å†…å­˜è®¿é—®

2. **å‘é‡åŒ–**ï¼š
   - ä½¿ç”¨ `fp16x2` æˆ– `fp16x4` å‘é‡ç±»å‹
   - ä¸€æ¬¡å¤„ç†å¤šä¸ªå…ƒç´ 

3. **å¿«é€Ÿè¿‘ä¼¼**ï¼š
   - å¯¹äºè¦æ±‚ä¸é«˜çš„åœºæ™¯ï¼Œå¯ä»¥ä½¿ç”¨ `expf` çš„å¿«é€Ÿè¿‘ä¼¼ç‰ˆæœ¬
   - AMD GPU æä¾› `__expf`ï¼ˆä½ç²¾åº¦å¿«é€Ÿç‰ˆæœ¬ï¼‰

---

## å…¶ä»– Activation æ”¯æŒ

åŸºäºå½“å‰å®ç°ï¼Œå¯ä»¥è½»æ¾æ·»åŠ å…¶ä»– activation å‡½æ•°ï¼š

### ReLU
```cpp
float relu_x = fmaxf(0.0f, x);
output[idx] = (fp16_t)relu_x;
```

### GELU
```cpp
// GELU(x) = x Â· Î¦(x), Î¦(x) æ˜¯æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„ CDF
// è¿‘ä¼¼ï¼šGELU(x) â‰ˆ 0.5 Â· x Â· (1 + tanh(sqrt(2/Ï€) Â· (x + 0.044715Â·xÂ³)))
float x3 = x * x * x;
float inner = 0.7978845608f * (x + 0.044715f * x3);  // sqrt(2/Ï€) â‰ˆ 0.7978845608
float gelu_x = 0.5f * x * (1.0f + tanhf(inner));
output[idx] = (fp16_t)gelu_x;
```

### Mish
```cpp
// Mish(x) = x Â· tanh(softplus(x)) = x Â· tanh(ln(1 + e^x))
float softplus_x = logf(1.0f + expf(x));
float mish_x = x * tanhf(softplus_x);
output[idx] = (fp16_t)mish_x;
```

---

## ä½¿ç”¨æŒ‡å—

### ç¼–è¯‘

```bash
cd /workspace/causal-conv1d/rocm_backend/matrix_core_opus

/opt/rocm/bin/hipcc -x hip -std=c++17 \
    casual_conv1d_opus.cpp \
    -o casual_conv1d_opus.exe \
    --offload-arch=gfx942 \
    -I/workspace/aiter/csrc/include \
    -I/root/libtorch/include \
    -I/root/libtorch/include/torch/csrc/api/include \
    -L/root/libtorch/lib \
    -Wl,-rpath=/root/libtorch/lib \
    -ltorch -lc10 -ltorch_cpu
```

### è¿è¡Œ

```bash
./casual_conv1d_opus.exe
```

### é…ç½®

åœ¨ `casual_conv1d_opus.cpp` ä¸­å¯ä»¥é…ç½®ä»¥ä¸‹å‚æ•°ï¼š

#### 1. å¯ç”¨/ç¦ç”¨ SiLU Activation

```cpp
#define ENABLE_SILU_ACTIVATION 1  // 1=å¯ç”¨ SiLU, 0=åªæ·»åŠ  bias
```

| å€¼ | è¡Œä¸º | ç”¨é€” |
|----|------|------|
| `1` | å¯ç”¨ SiLU | Conv1D + Bias + SiLUï¼ˆé»˜è®¤ï¼‰ |
| `0` | ç¦ç”¨ SiLU | Conv1D + Biasï¼ˆæ—  activationï¼‰ |

**æµ‹è¯•è¾“å‡ºå¯¹æ¯”ï¼š**

```bash
# ENABLE_SILU_ACTIVATION = 1
åœ¨ GPU ä¸Šæ·»åŠ  bias å¹¶åº”ç”¨ SiLU activation (batch=1)...
âœ“ bias + SiLU activation å®Œæˆ

# ENABLE_SILU_ACTIVATION = 0
åœ¨ GPU ä¸Šæ·»åŠ  bias (batch=1)...
âœ“ bias æ·»åŠ å®Œæˆ
```

**åº”ç”¨åœºæ™¯ï¼š**
- æ€§èƒ½å¯¹æ¯”æµ‹è¯•ï¼šæ¯”è¾ƒæœ‰æ—  activation çš„æ€§èƒ½å·®å¼‚
- åŠŸèƒ½éªŒè¯ï¼šéªŒè¯ activation å¯¹ç²¾åº¦çš„å½±å“
- è°ƒè¯•ï¼šç®€åŒ–è®¡ç®—æµç¨‹ï¼Œä¾¿äºé—®é¢˜å®šä½

#### 2. æ‰¹å¤„ç†å¤§å°

```cpp
int batch = 1;  // æ”¯æŒ batch >= 1
```

#### 3. Host ç«¯éªŒè¯

```cpp
#define ENABLE_HOST_VERIFICATION 1  // 1=å¯ç”¨éªŒè¯, 0=çº¯GPUæ¨¡å¼
```

---

## æ€»ç»“

### å®ç°è¦ç‚¹

âœ… **èåˆ Kernel**ï¼šBias + SiLU èåˆï¼Œå‡å°‘å†…å­˜è®¿é—®  
âœ… **æ•°å€¼ç¨³å®š**ï¼šä½¿ç”¨ FP32 è¿›è¡Œä¸­é—´è®¡ç®—  
âœ… **æ‰¹å¤„ç†æ”¯æŒ**ï¼šæ”¯æŒ `batch > 1`  
âœ… **éªŒè¯é€šè¿‡**ï¼šHost å’Œ GPU ç»“æœä¸€è‡´  

### æ€§èƒ½ç‰¹ç‚¹

- **è®¡ç®—å¼€é”€**ï¼šç›¸å¯¹äº GEMM < 5%
- **å†…å­˜è®¿é—®**ï¼šèåˆåä»…å¢åŠ  1 æ¬¡è¯»ï¼ˆbiasï¼‰
- **å¯æ‰©å±•æ€§**ï¼šæ˜“äºæ·»åŠ å…¶ä»– activation å‡½æ•°

### æœªæ¥ä¼˜åŒ–

1. èåˆåˆ° GEMM kernel
2. å‘é‡åŒ–å¤„ç†
3. ä½¿ç”¨å¿«é€Ÿæ•°å­¦åº“å‡½æ•°

---

**æ–‡æ¡£ç‰ˆæœ¬ï¼š** 1.0  
**æœ€åæ›´æ–°ï¼š** 2025-11-15  
**ä½œè€…ï¼š** AI Assistant  
**ç›¸å…³æ–‡ä»¶ï¼š** `casual_conv1d_opus.cpp`



